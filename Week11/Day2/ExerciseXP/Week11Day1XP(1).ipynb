{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Loan Default Prediction Project\n",
        "\n",
        "#Defining the Problem and Data Collection (Exercise 1)\n",
        "\n",
        "## Problem Statement: Predict the likelihood for incoming loan applications, that if approved, will result in default, using historical data.\n",
        "\n",
        "## Data Types: Accumulate data on the number of loan applications, the applicant's demographic and financial profile, the requested loan profile (date, amount, loan structure, purpose) , the application result, and for those approved, a historical timeline of payment and closure.\n",
        "\n",
        "## Data Collection: Data will be obtained via our internal systems, credit bureaus, and available government records.\n",
        "\n",
        "#Feature Selection and Model Choice (Exercise 2)\n",
        "\n",
        "## Projected Relevant Features and Justification\n",
        "\n",
        "### Demographic Information (Gender, Married, Dependents, Education, Self-Employed, Property Area): Useful for identifying trends/correlations between applicant's profile and level of risk.\n",
        "\n",
        "### Financial Information (Applicant Income, Coapplicant Income, Credit History): Useful for comparing current income to determine ability to pay, and to assess the applicant's financial history, past performance, including other debt to assess their behavior.\n",
        "\n",
        "### Loan Information (Loan Amount, Loan Amount Term, Loan Status) When connected to the other two categories, it is useful to draw correlations.\n",
        "\n",
        "#Training, Evaluating, and Optimizing the Model (Exercise 3)\n",
        "\n",
        "## Model: I will implement the classification model to predict which applicants are most likely to result in a loan default.\n",
        "\n",
        "## Training: In order to train the model, I will use the K-Fold Cross-Validation. This is because I want to ensure that we have enough data subsets to train the model, evaluate performance against another subset, and continuously optimize it until we reach an acceptable threshold. I will also use the F-1 Score because I am not sure if there's an uneven distribution between applications likely to default vs. those who are qualified and could be rejected. Finally, for the loss function, I’ll use the Binary Crossentropy to see how well the predicted probabilities performed against what really happened using the historical data subsets.\n",
        "\n",
        "\n",
        "# Designing Machine Learning Solutions (Exercise 4)\n",
        "\n",
        "## Predicting future stock prices: Supervised Learning will enable us to use the labeled data to predict a numerical value and classify the data into categories. We can use the dataset to give the algorithm direct feedback.\n",
        "\n",
        "## Organizing a Library of Books into genres or categories based on similarities. As with the prior, Supervised Learning based on genre and other similar categories (book type, author, topic, etc), will enable us to classify the data into categories - in other words, organizing the library.\n",
        "\n",
        "## Program a robot to navigate and find the shortest path in a maze: I'd use Reinforcement Learning, because the robot learns by exploring the maze and receiving rewards or penalties depending on its actions. Since the robot doesn't have labeled data in advance; the machine learns through consequences to find the shortest path.\n",
        "\n",
        "# Designing an Evaluation Strategy (Exercise 5)\n",
        "\n",
        "## Supervised Learning: My metrics will include accuracy, precision / recall, and the F1-score so that I can train the model on historical data and compare performance against actuals. I'd also use the K-Fold Cross Validation to determine performance.\n",
        "\n",
        "## Unsupervised Learning: I’d look at the silhouette score or use the elbow method to see if the clusters make sense. Not sure how I will comopare without a historical dataset.\n",
        "\n",
        "## Reinforcement Learning: I’d measure how much reward it earns over time and check if it’s improving or converging with a good strategy. Not sure how I will select the right one.  \n"
      ],
      "metadata": {
        "id": "9E2ArH3Xqv0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Loan Default Prediction Project\n",
        "\n",
        "#Defining the Problem and Data Collection (Exercise 1)\n",
        "\n",
        "## Problem Statement: Predict the likelihood for incoming loan applications, that if approved, will result in default, using historical data.\n",
        "\n",
        "## Data Types: Accumulate data on the number of loan applications, the applicant's demographic and financial profile, the requested loan profile (date, amount, loan structure, purpose) , the application result, and for those approved, a historical timeline of payment and closure.\n",
        "\n",
        "## Data Collection: Data will be obtained via our internal systems, credit bureaus, and available government records.\n",
        "\n",
        "data_types =\n",
        "[\"number of loan applications\", \"applicant's demographic profile\", \"applicant's financial profile\", \"requested loan profile (date, amount, structure, purpose)\", \"application result\", \"for approved loans: timeline of payments and closure\"]\n",
        "\n",
        "data_sources =\n",
        "[\"our internal systems (applications, servicing, collections)\", \"credit bureaus\", \"available government records / public economic indicators\"]\n",
        "\n",
        "print(problem_statement)\n",
        "print(\"Data Types:\", data_types)\n",
        "print(\"Data Sources:\", data_sources)\n",
        "\n",
        "#Feature Selection and Model Choice (Exercise 2)\n",
        "\n",
        "## Projected Relevant Features and Justification\n",
        "\n",
        "### Demographic Information (Gender, Married, Dependents, Education, Self-Employed, Property Area): Useful for identifying trends/correlations between applicant's profile and level of risk.\n",
        "\n",
        "### Financial Information (Applicant Income, Coapplicant Income, Credit History): Useful for comparing current income to determine ability to pay, and to assess the applicant's financial history, past performance, including other debt to assess their behavior.\n",
        "\n",
        "### Loan Information (Loan Amount, Loan Amount Term, Loan Status) When connected to the other two categories, it is useful to draw correlations.\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rng = np.random.default_rng(11)\n",
        "if USE_SYNTHETIC:\n",
        "    n = 2500\n",
        "    df = pd.DataFrame({\n",
        "        'Gender': rng.choice(['Male','Female'], size=n),\n",
        "        'Married': rng.choice(['Yes','No'], size=n, p=[0.6,0.4]),\n",
        "        'Dependents': rng.integers(0,4, size=n),\n",
        "        'Education': rng.choice(['Graduate','Not Graduate'], size=n, p=[0.7,0.3]),\n",
        "        'Self_Employed': rng.choice(['Yes','No'], size=n, p=[0.15,0.85]),\n",
        "        'Property_Area': rng.choice(['Urban','Semiurban','Rural'], size=n, p=[0.4,0.35,0.25]),\n",
        "        'ApplicantIncome': np.clip(rng.normal(5000, 2000, size=n), 500, 20000).round(0),\n",
        "        'CoapplicantIncome': np.clip(rng.normal(1500, 1000, size=n), 0, 15000).round(0),\n",
        "        'Credit_History': rng.choice([1.0, 0.0, np.nan], size=n, p=[0.75, 0.2, 0.05]),\n",
        "        'LoanAmount': np.clip(rng.normal(150, 60, size=n), 20, 600).round(1),\n",
        "        'Loan_Amount_Term': rng.choice([180,240,300,360], size=n, p=[0.1,0.2,0.3,0.4]),\n",
        "        'Loan_Status': rng.choice(['Y','N'], size=n, p=[0.85,0.15]),\n",
        "        'Purpose': rng.choice(['Home','Car','Education','Personal'], size=n, p=[0.5,0.2,0.15,0.15])\n",
        "    })\n",
        "    logit = (\n",
        "        -3.2 - 0.00022*df['ApplicantIncome'] - 0.0001*df['CoapplicantIncome']\n",
        "        - 1.0*df['Credit_History'].fillna(0) + 0.004*df['LoanAmount']\n",
        "        + 0.15*(df['Purpose'].isin(['Personal','Education']).astype(int))\n",
        "    )\n",
        "    p = 1/(1+np.exp(-logit))\n",
        "    df['Default'] = rng.binomial(1, p)\n",
        "else:\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    # Expect a binary target column named 'Default' (0/1).\n",
        "\n",
        "y = df['Default']\n",
        "X = df.drop(columns=['Default'])\n",
        "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
        "num_cols = X.select_dtypes(include=['number','bool']).columns.tolist()\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_cols),\n",
        "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"oh\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols)\n",
        "])\n",
        "\n",
        "# Fit transform once for feature scoring\n",
        "Xenc = preprocess.fit_transform(X)\n",
        "mi = mutual_info_classif(Xenc, y, random_state=11)\n",
        "ohe = preprocess.named_transformers_['cat']['oh'] if cat_cols else None\n",
        "feat_names = num_cols + (ohe.get_feature_names_out(cat_cols).tolist() if ohe is not None else [])\n",
        "import pandas as pd\n",
        "mi_series = pd.Series(mi, index=feat_names).sort_values(ascending=False)\n",
        "print(\"Top features by Mutual Information (filter):\\n\", mi_series.head(12))\n",
        "\n",
        "rf = Pipeline([('prep', preprocess), ('rf', RandomForestClassifier(n_estimators=250, random_state=11, class_weight='balanced'))])\n",
        "rf.fit(X, y)\n",
        "rf_imp = pd.Series(rf.named_steps['rf'].feature_importances_, index=feat_names).sort_values(ascending=False)\n",
        "print(\"\\nTop features by RandomForest importance (embedded):\\n\", rf_imp.head(12))\n",
        "\n",
        "#Training, Evaluating, and Optimizing the Model (Exercise 3)\n",
        "\n",
        "## Model: I will implement the classification model to predict which applicants are most likely to result in a loan default.\n",
        "\n",
        "## Training: In order to train the model, I will use the K-Fold Cross-Validation. This is because I want to ensure that we have enough data subsets to train the model, evaluate performance against another subset, and continuously optimize it until we reach an acceptable threshold. I will also use the F-1 Score because I am not sure if there's an uneven distribution between applications likely to default vs. those who are qualified and could be rejected. Finally, for the loss function, I’ll use the Binary Crossentropy to see how well the predicted probabilities performed against what really happened using the historical data subsets.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, average_precision_score, log_loss)\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=11)\n",
        "\n",
        "def pipe(model):\n",
        "    return Pipeline([('prep', preprocess), ('clf', model)])\n",
        "\n",
        "models = {\n",
        "    'LogReg': LogisticRegression(max_iter=300, class_weight='balanced'),\n",
        "    'RF': RandomForestClassifier(n_estimators=300, random_state=11, class_weight='balanced')\n",
        "}\n",
        "\n",
        "for name, mdl in models.items():\n",
        "    p = pipe(mdl)\n",
        "    p.fit(X_train, y_train)\n",
        "    yp = p.predict(X_test)\n",
        "    yproba = p.predict_proba(X_test)[:,1]\n",
        "    print(f\"\\n=== {name} (Test) ===\")\n",
        "    print(\"Accuracy:\", round(accuracy_score(y_test, yp), 3))\n",
        "    print(\"Precision:\", round(precision_score(y_test, yp), 3))\n",
        "    print(\"Recall:\", round(recall_score(y_test, yp), 3))\n",
        "    print(\"F1:\", round(f1_score(y_test, yp), 3))\n",
        "    print(\"ROC-AUC:\", round(roc_auc_score(y_test, yproba), 3))\n",
        "    print(\"PR-AUC:\", round(average_precision_score(y_test, yproba), 3))\n",
        "    print(\"Log Loss:\", round(log_loss(y_test, yproba), 3))\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)\n",
        "    cv = cross_validate(p, X, y, cv=skf, scoring=['f1','roc_auc','average_precision'])\n",
        "    print(\"CV F1 (mean±std): {:.3f}±{:.3f}\".format(cv['test_f1'].mean(), cv['test_f1'].std()))\n",
        "    print(\"CV ROC-AUC (mean±std): {:.3f}±{:.3f}\".format(cv['test_roc_auc'].mean(), cv['test_roc_auc'].std()))\n",
        "    print(\"CV PR-AUC (mean±std): {:.3f}±{:.3f}\".format(cv['test_average_precision'].mean(), cv['test_average_precision'].std()))\n",
        "\n",
        "# Designing Machine Learning Solutions (Exercise 4)\n",
        "\n",
        "## Predicting future stock prices: Supervised Learning will enable us to use the labeled data to predict a numerical value and classify the data into categories. We can use the dataset to give the algorithm direct feedback.\n",
        "\n",
        "## Organizing a Library of Books into genres or categories based on similarities. As with the prior, Supervised Learning based on genre and other similar categories (book type, author, topic, etc), will enable us to classify the data into categories - in other words, organizing the library.\n",
        "\n",
        "## Program a robot to navigate and find the shortest path in a maze: I'd use Reinforcement Learning, because the robot learns by exploring the maze and receiving rewards or penalties depending on its actions. Since the robot doesn't have labeled data in advance; the machine learns through consequences to find the shortest path.\n",
        "\n",
        "# Designing an Evaluation Strategy (Exercise 5)\n",
        "\n",
        "## Supervised Learning: My metrics will include accuracy, precision / recall, and the F1-score so that I can train the model on historical data and compare performance against actuals. I'd also use the K-Fold Cross Validation to determine performance.\n",
        "\n",
        "## Unsupervised Learning: I’d look at the silhouette score or use the elbow method to see if the clusters make sense. Not sure how I will comopare without a historical dataset.\n",
        "\n",
        "## Reinforcement Learning: I’d measure how much reward it earns over time and check if it’s improving or converging with a good strategy. Not sure how I will select the right one.\n",
        "\n",
        "\n",
        "# ---- Unsupervised demo: K-Means with silhouette + elbow ----\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Xb, _ = make_blobs(n_samples=900, centers=4, cluster_std=0.7, random_state=21)\n",
        "ks = list(range(2,9))\n",
        "inertias, sils = [], []\n",
        "for k in ks:\n",
        "    km = KMeans(n_clusters=k, n_init=10, random_state=21)\n",
        "    km.fit(Xb)\n",
        "    inertias.append(km.inertia_)\n",
        "    sils.append(silhouette_score(Xb, km.labels_))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(ks, inertias, marker='o')\n",
        "plt.xlabel('k'); plt.ylabel('Inertia'); plt.title('Elbow Method (K-Means)'); plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(ks, sils, marker='o')\n",
        "plt.xlabel('k'); plt.ylabel('Silhouette'); plt.title('Silhouette vs k'); plt.show()\n",
        "\n",
        "print('Best k by silhouette:', ks[int(np.argmax(sils))])\n",
        "\n",
        "# ---- RL demo: simple epsilon-greedy bandit to visualize cumulative reward ----\n",
        "import numpy as np\n",
        "rng = np.random.default_rng(21)\n",
        "k = 5\n",
        "true_means = rng.uniform(0.1, 0.9, size=k)\n",
        "best_mean = true_means.max()\n",
        "def run_bandit(eps=0.1, steps=1500):\n",
        "    q = np.zeros(k); n = np.zeros(k); rewards = []\n",
        "    for t in range(steps):\n",
        "        a = rng.integers(0,k) if rng.random()<eps else int(np.argmax(q))\n",
        "        r = rng.normal(true_means[a], 0.1)\n",
        "        n[a]+=1; q[a]+= (r-q[a])/n[a]\n",
        "        rewards.append(r)\n",
        "    return np.array(rewards)\n",
        "r01 = run_bandit(0.1)\n",
        "r00 = run_bandit(0.0)\n",
        "cum_opt = np.arange(1,len(r01)+1)*best_mean\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(); plt.plot(r01.cumsum(), label='eps=0.1'); plt.plot(r00.cumsum(), label='eps=0.0'); plt.plot(cum_opt, label='optimal');\n",
        "plt.xlabel('Steps'); plt.ylabel('Cumulative Reward'); plt.title('Bandit: Cumulative Reward'); plt.legend(); plt.show()\n",
        "print('True arm means:', np.round(true_means,3))\n",
        "\n"
      ],
      "metadata": {
        "id": "1Eso6wn7p2j6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}