{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Setup: Install and configure Kaggle API\n",
        "!pip install -q kaggle\n",
        "#Installs the kaggle Python package so you can access datasets directly\n",
        "# from Kaggle competitions using code.\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "#\n",
        "!kaggle competitions download -c titanic\n",
        "!unzip -q titanic.zip\n"
      ],
      "metadata": {
        "id": "akGvK2SHhc46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        " # Upload kaggle.json# ✅ 2. Import libraries and load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "titanic_data = pd.read_csv('train.csv')\n",
        "print(titanic_data.head(6))\n",
        "\n"
      ],
      "metadata": {
        "id": "qLcvjns1hc_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic_data.info())\n"
      ],
      "metadata": {
        "id": "1RSc1GM775gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic_data.describe())"
      ],
      "metadata": {
        "id": "aNS71ZDp8Bt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cOME1wbm78Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate extra demographic data (for integration example)\n",
        "demographics = titanic_data[['PassengerId']].copy()\n",
        "demographics['Has_Pets'] = np.random.choice([True, False], size=len(demographics))\n",
        "\n",
        "# Merge\n",
        "titanic_data = pd.merge(titanic_data, demographics, on='PassengerId', how='left')\n",
        "print(titanic_data[['PassengerId', 'Has_Pets']].head())\n",
        "\n",
        "# The LEFT JOIN keyword returns all records from the left table (table1), and the matching records from the right table (table2).\n"
      ],
      "metadata": {
        "id": "0XrkCurlhdDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2.Add a duplicate of the first row\n",
        "titanic_data = pd.concat([titanic_data, titanic_data.iloc[[0]]], ignore_index=True)\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicate_mask = titanic_data.duplicated()\n",
        "\n",
        "# Print number of duplicate rows\n",
        "print(\"Number of duplicate rows:\", duplicate_mask.sum())\n",
        "\n",
        "# Optionally, view the duplicated rows\n",
        "print(titanic_data[duplicate_mask])\n"
      ],
      "metadata": {
        "id": "KxtUXAOHlYWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVGqi5yHBvo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Duplicates\n",
        "titanic_data = titanic_data.drop_duplicates()\n",
        "print(\"Remaining duplicates:\", titanic_data.duplicated().sum())"
      ],
      "metadata": {
        "id": "swzPFshfBn-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove Irrelevant Columns\n",
        "titanic_data = titanic_data.drop(['Cabin', 'Ticket'], axis=1, errors='ignore')\n",
        "print(\"Columns after removal:\", titanic_data.columns)"
      ],
      "metadata": {
        "id": "sGmji09oB4J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing Structural Errors - simulate 'Date' column\n",
        "titanic_data['Date'] = pd.date_range(start='1/1/1912', periods=len(titanic_data), freq='D')\n",
        "titanic_data['Date'] = pd.to_datetime(titanic_data['Date'])\n",
        "print(titanic_data[['Date']].head())\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nYtjnvcUDpFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Check for missing values in a DataFrame\n",
        "missing_data = titanic_data.isnull()\n",
        "print(missing_data.head())\n",
        "\n",
        "# Count missing values in each column\n",
        "missing_counts = titanic_data.isnull().sum()\n",
        "print(missing_counts)"
      ],
      "metadata": {
        "id": "LNYOF0S6e6G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Identify Missing\n",
        "print(\"Missing values per column:\")\n",
        "print(titanic_data.isnull().sum())"
      ],
      "metadata": {
        "id": "SDe_LCcxC2MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify Missing\n",
        "print(\"Missing values per column:\")\n",
        "print(titanic_data.isnull().sum())\n",
        "\n",
        "# Fill with mean\n",
        "titanic_filled = titanic_data.copy()\n",
        "titanic_filled['Age'] = titanic_filled['Age'].fillna(titanic_filled['Age'].mean())\n",
        "print(\"Nulls in Age after filling:\", titanic_filled['Age'].isnull() .sum())\n",
        "\n",
        "\n",
        "# Drop rows with missing values\n",
        "titanic_dropna = titanic_data.dropna()\n",
        "print(\"Shape after dropping rows with missing data:\", titanic_dropna.shape)\n"
      ],
      "metadata": {
        "id": "DQlw6QrmiYGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NTn-FvesnEkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNNImputer is a technique for filling in missing values based on the values of the k-nearest neighbors (rows that are similar in other features). It’s more intelligent than filling with mean or median, because it accounts for relationships between features.\n",
        "\n",
        "✅ Create the imputer object.\n",
        "\n",
        "n_neighbors=3 means for each missing value, the algorithm will:\n",
        "\n",
        "Find the 3 rows most similar to the one with missing data (based on other numeric columns).\n",
        "\n",
        "Use their values to compute the average and fill in the missing value.\n",
        "\n",
        "\n",
        " Filter only numeric columns, because KNNImputer can only work with numbers.\n",
        "This ensures we don’t feed in strings or categories like 'Sex' or 'Embarked'.\n",
        "\n",
        "\n",
        "Perform the imputation:\n",
        "\n",
        "fit_transform() does two things:\n",
        "\n",
        "Fit: Learns how to find neighbors based on available (non-missing) data.\n",
        "\n",
        "Transform: Fills in the missing values using neighbor-based averages.\n",
        "\n",
        "The result is a NumPy array with all missing values filled.\n",
        "\n",
        "✅ Convert the result back to a DataFrame, keeping the original column names.\n",
        "\n",
        "✅ Just to check: Print the first few rows of the cleaned, imputed data."
      ],
      "metadata": {
        "id": "n3af6bsNnE0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "df_numeric = titanic_data.select_dtypes(include=['float64', 'int64'])\n",
        "imputed_array = imputer.fit_transform(df_numeric)\n",
        "df_imputed = pd.DataFrame(imputed_array, columns=df_numeric.columns)\n",
        "print(df_imputed.head())\n"
      ],
      "metadata": {
        "id": "ilVnS2Npiz4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = titanic_data['Fare'].quantile(0.25)\n",
        "Q3 = titanic_data['Fare'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "filtered_fare = titanic_data[(titanic_data['Fare'] >= lower_bound) & (titanic_data['Fare'] <= upper_bound)]\n",
        "print(\"Original shape:\", titanic_data.shape)\n",
        "print(\"Shape after removing fare outliers:\", filtered_fare.shape)\n"
      ],
      "metadata": {
        "id": "k8wpKHEWi4HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What’s going on?\n",
        "Q1 (25th percentile) is the value below which 25% of the data falls.\n",
        "\n",
        "Q3 (75th percentile) is the value below which 75% of the data falls.\n",
        "\n",
        "IQR (Interquartile Range) is the difference between Q3 and Q1:\n",
        "\n",
        "IQR\n",
        "=\n",
        "𝑄\n",
        "3\n",
        "−\n",
        "𝑄\n",
        "1\n",
        "IQR=Q3−Q1\n",
        "It measures the spread of the middle 50% of the data.\n",
        "\n",
        "\n",
        "What’s going on?\n",
        "These formulas define the acceptable range for Age.\n",
        "\n",
        "Any value outside this range is considered an outlier.\n",
        "\n",
        "Lower Bound\n",
        "=\n",
        "𝑄\n",
        "1\n",
        "−\n",
        "1.5\n",
        "×\n",
        "𝐼\n",
        "𝑄\n",
        "𝑅\n",
        "Lower Bound=Q1−1.5×IQR\n",
        "Upper Bound\n",
        "=\n",
        "𝑄\n",
        "3\n",
        "+\n",
        "1.5\n",
        "×\n",
        "𝐼\n",
        "𝑄\n",
        "𝑅\n",
        "Upper Bound=Q3+1.5×IQR\n",
        "This \"1.5 * IQR\" rule is a common statistical convention:\n",
        "\n",
        "Too far below = unusually small → outlier\n",
        "\n",
        "Too far above = unusually large → outlier"
      ],
      "metadata": {
        "id": "_bam3ftdoe0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Q1 and Q3\n",
        "Q1_age = titanic_data['Age'].quantile(0.25)\n",
        "Q3_age = titanic_data['Age'].quantile(0.75)\n",
        "IQR_age = Q3_age - Q1_age\n",
        "\n",
        "# 2. Bounds\n",
        "lower_age = Q1_age - 1.5 * IQR_age\n",
        "upper_age = Q3_age + 1.5 * IQR_age\n",
        "\n",
        "# 3. Count Outliers\n",
        "age_outliers = titanic_data[(titanic_data['Age'] < lower_age) | (titanic_data['Age'] > upper_age)]\n",
        "print(\"Number of Age outliers:\", age_outliers.shape[0])\n",
        "print(age_outliers[['Age']].head())\n"
      ],
      "metadata": {
        "id": "TXQhy3Uki93s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a sample dataset\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'CustomerID': range(1, 11),\n",
        "    'Age': np.random.randint(18, 70, 10),\n",
        "    'Annual_Income': np.random.randint(30000, 120000, 10),\n",
        "    'Spending_Score': np.random.randint(1, 100, 10),\n",
        "    'Purchase_Amount': np.random.randint(100, 1000, 10),\n",
        "    'Purchase_Date': pd.date_range(start='2024-01-01', periods=10, freq='15D')\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "co8pgFUxp5DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 1. Data Normalization\n",
        "What it is: Rescaling features to a standard range (typically 0 to 1) so that no feature dominates others due to its scale.\n",
        "\n",
        "Why it's important: Algorithms like KNN, clustering, and PCA are sensitive to scale — normalization helps them treat all features equally.\n",
        "\n",
        "In the example: We normalized Age, Annual_Income, and Spending_Score using MinMaxScaler to create new columns ending in _norm.\n",
        "\n"
      ],
      "metadata": {
        "id": "3sAmRH_dqpox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Data Normalization (Min-Max Scaling)\n",
        "scaler = MinMaxScaler()\n",
        "df[['Age_norm', 'Annual_Income_norm', 'Spending_Score_norm']] = scaler.fit_transform(\n",
        "    df[['Age', 'Annual_Income', 'Spending_Score']]\n",
        ")\n",
        "df\n"
      ],
      "metadata": {
        "id": "n_Jtpy52qAlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 2. Data Reduction using PCA\n",
        "What it is: Reduces the number of columns (features) while keeping most of the information.\n",
        "\n",
        "Why it's important: Makes data easier to visualize and can improve performance in high-dimensional datasets.\n",
        "\n",
        "In the example: We used PCA to reduce the normalized features to two components (PCA1, PCA2) — these represent the most meaningful variance in the data."
      ],
      "metadata": {
        "id": "bDqiTbmvqq7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Data Reduction using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(df[['Age_norm', 'Annual_Income_norm', 'Spending_Score_norm']])\n",
        "df[['PCA1', 'PCA2']] = pca_features\n",
        "df"
      ],
      "metadata": {
        "id": "1DulHz-AqLil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "_v6D4fPEsvX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting PCA1 vs PCA2 with labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['PCA1'], df['PCA2'], c='blue', edgecolors='k', s=80)\n",
        "plt.title('PCA: Data Projection onto 2 Principal Components')\n",
        "plt.xlabel('Principal Component 1 (PCA1)')\n",
        "plt.ylabel('Principal Component 2 (PCA2)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fwMMjr9NtH8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 3. Data Aggregation\n",
        "What it is: Summarizing data — typically grouping by a category or time period and applying aggregation functions (e.g., sum, mean).\n",
        "\n",
        "Why it's important: Allows simplification of raw data for trend analysis, reporting, or dashboard creation.\n",
        "\n",
        "In the example: We aggregated the Purchase_Amount by month using .groupby() to see how total purchases vary over time."
      ],
      "metadata": {
        "id": "AtWUL9tOrHJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Data Aggregation - Group by Month and calculate total Purchase_Amount\n",
        "df['Month'] = df['Purchase_Date'].dt.to_period('M')\n",
        "monthly_agg = df.groupby(['Month', 'CustomerID']).agg(\n",
        "    Purchase_Amount_min=('Purchase_Amount', 'min'),\n",
        "    Purchase_Amount_max=('Purchase_Amount', 'max')\n",
        ").reset_index()\n",
        "\n",
        "print(monthly_agg)\n"
      ],
      "metadata": {
        "id": "Cw8LQhHFqLqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 1. Data Normalization\n",
        "What it is: Rescaling features to a standard range (typically 0 to 1) so that no feature dominates others due to its scale.\n",
        "\n",
        "Why it's important: Algorithms like KNN, clustering, and PCA are sensitive to scale — normalization helps them treat all features equally.\n",
        "\n",
        "In the example: We normalized Age, Annual_Income, and Spending_Score using MinMaxScaler to create new columns ending in _norm.\n",
        "\n"
      ],
      "metadata": {
        "id": "84sJkAj7qbdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        " # Upload kaggle.json# ✅ 2. Import libraries and load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "weather_data = pd.read_csv('weatherHistory.csv')\n",
        "print(weather_data.head(6))\n",
        "\n"
      ],
      "metadata": {
        "id": "VXrdft5tRch0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(weather_data.info())"
      ],
      "metadata": {
        "id": "csIv8PhmZw3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(weather_data.describe())"
      ],
      "metadata": {
        "id": "zQ09CCEmZxnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data.head()"
      ],
      "metadata": {
        "id": "ynPQQ4RPZ1yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Normalization (Min-Max Scaling)\n",
        "scaler = MinMaxScaler()\n",
        "weather_data[['Temperature (C)_norm', 'Wind Speed (km/h)_norm', 'Pressure (millibars)_norm']] = scaler.fit_transform(\n",
        "    weather_data[['Temperature (C)', 'Wind Speed (km/h)', 'Pressure (millibars)']]\n",
        ")\n",
        "weather_data"
      ],
      "metadata": {
        "id": "1Z9oIjktZ42S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data Reduction using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(weather_data[['Temperature (C)_norm', 'Wind Speed (km/h)_norm', 'Pressure (millibars)_norm']])\n",
        "weather_data[['PCA1', 'PCA2']] = pca_features\n",
        "weather_data"
      ],
      "metadata": {
        "id": "MVpxbz3uZ8_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plotting PCA1 vs PCA2 with labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(weather_data['PCA1'], weather_data['PCA2'], c='blue', edgecolors='k', s=80)\n",
        "plt.title('PCA: Data Projection onto 2 Principal Components')\n",
        "plt.xlabel('Principal Component 1 (PCA1)')\n",
        "plt.ylabel('Principal Component 2 (PCA2)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R080Lef4aBHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 3. Data Aggregation - Group by Month and calculate the average Temperature (C)\n",
        "weather_data['Formatted Date'] = pd.to_datetime(weather_data['Formatted Date'], utc=True)\n",
        "weather_data['Month'] = weather_data['Formatted Date'].dt.to_period('M')\n",
        "\n",
        "daily_agg = weather_data.groupby('Month').agg({'Temperature (C)': 'mean'})\n",
        "daily_agg\n"
      ],
      "metadata": {
        "id": "Odq3prI-aHKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# 1. Data Normalization\n",
        "scaler = MinMaxScaler()\n",
        "numerical_cols = weather_data.select_dtypes(include='number').columns\n",
        "weather_data_normalized = scaler.fit_transform(weather_data[numerical_cols])\n",
        "weather_data[[col + '_norm' for col in numerical_cols]] = pd.DataFrame(weather_data_normalized, columns=[col + '_norm' for col in numerical_cols])\n",
        "weather_data\n",
        "\n",
        "# 2. PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(weather_data[[col + '_norm' for col in numerical_cols]])\n",
        "weather_data[['PCA1', 'PCA2']] = pca_result\n",
        "\n",
        "# 3. Aggregation: Average temperature and humidity by summary\n",
        "weather_agg = weather_data.groupby('Daily Summary').agg(\n",
        "    Avg_Temp=('Apparent Temperature (C)', 'mean'),\n",
        "    Avg_Humidity=('Humidity', 'mean')\n",
        ").reset_index()\n",
        "\n"
      ],
      "metadata": {
        "id": "ubOkXigWStTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}